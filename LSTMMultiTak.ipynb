{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DefaultConfig.INI']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import configparser\n",
    "sys.path.insert(0, 'Utils/')\n",
    "\n",
    "from AISDataManager import AISDataManager\n",
    "import SimpleUtils as sU\n",
    "import Constants as c\n",
    "import TimeUtils as timeUtils\n",
    "import HMUtils as hMUtil\n",
    "import GeoCompute as gC\n",
    "\n",
    "aISDM = AISDataManager()\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('DefaultConfig.INI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-122.0 33.4\n",
      "-118.5 36.4\n"
     ]
    }
   ],
   "source": [
    "lonMin = (float)(config['LSTM_MULTI_TASK_TRAIN']['LON_MIN'])\n",
    "lonMax = (float)(config['LSTM_MULTI_TASK_TRAIN']['LON_MAX'])\n",
    "\n",
    "latMin = (float)(config['LSTM_MULTI_TASK_TRAIN']['LAT_MIN'])\n",
    "latMax = (float)(config['LSTM_MULTI_TASK_TRAIN']['LAT_MAX'])\n",
    "\n",
    "print(lonMin,latMin)\n",
    "print(lonMax,latMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VesselTypeSource:\n",
    "    \"\"\"\n",
    "    The VesselTypeSource object contains information. \n",
    "    which is useful for the training data\n",
    "    \"\"\"\n",
    "    def __init__(self, srcDir, typeVes):\n",
    "        self.srcDir = srcDir\n",
    "        self.type = typeVes\n",
    "        self.trainList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jcharla/LiporLab/Data/M122_00_M118_50_33_40_36_40/MMSI_1004_TRAIN_INTP/\n",
      "/home/jcharla/LiporLab/Data/M122_00_M118_50_33_40_36_40/MMSI_1024_TRAIN_INTP/\n",
      "/home/jcharla/LiporLab/Data/M122_00_M118_50_33_40_36_40/Models/TS_5/\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "sourceDir1 = config['LSTM_MULTI_TASK_TRAIN']['SRC_DIR_1']\n",
    "sourceDir2 = config['LSTM_MULTI_TASK_TRAIN']['SRC_DIR_2']\n",
    "\n",
    "modelDir = config['LSTM_MULTI_TASK_TRAIN']['MODEL_DIR']\n",
    "prevTS = int(config['LSTM_MULTI_TASK_TRAIN']['PREV_TS'])\n",
    "\n",
    "print(sourceDir1)\n",
    "print(sourceDir2)\n",
    "print(modelDir)\n",
    "print(prevTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.VesselTypeSource object at 0x7f3974383240>\n",
      "<__main__.VesselTypeSource object at 0x7f3974383208>\n"
     ]
    }
   ],
   "source": [
    "vesselSource1 = VesselTypeSource(sourceDir1,0)\n",
    "vesselSource2 = VesselTypeSource(sourceDir2,1)\n",
    "\n",
    "vesselDataSources = [vesselSource1, vesselSource2]\n",
    "print(vesselDataSources[0])\n",
    "print(vesselDataSources[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_traj_data(fileName):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(fileName)\n",
    "    return sourceDF.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seq_to_x_y(seq, vType, prevTimeStamp):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #-prevTimeStamp is is to take care of boundary condition\n",
    "    #since we are considering prevTimeStamp time stamps for the input data\n",
    "    xNumRows = seq[:-(prevTimeStamp),:].shape[0]\n",
    "    print(xNumRows)\n",
    "    dateStr = seq[0][0]\n",
    "    monF = int(dateStr.split(\"-\")[1])\n",
    "    print(monF)\n",
    "    monData = np.zeros((xNumRows, 12))\n",
    "    monData[:,(12-monF)] = 1\n",
    "    \n",
    "    typeArr = np.zeros((xNumRows,2))\n",
    "    typeArr[:,vType] = 1\n",
    "    \n",
    "    lonLatColList = []\n",
    "    for start in range(prevTimeStamp):\n",
    "        lonLatColList.append(seq[start:(-prevTimeStamp+start),1:].copy())\n",
    "        \n",
    "    outputLabelLonLat = seq[prevTimeStamp:,1:].copy()\n",
    "    \n",
    "    outputLabel = (outputLabelLonLat)\n",
    "    \n",
    "    xDataTS = np.zeros((xNumRows,0))\n",
    "    \n",
    "    for tS in range(prevTimeStamp):\n",
    "        xDataTS = np.hstack((xDataTS,lonLatColList[tS]))\n",
    "        xDataTS = np.hstack((xDataTS,typeArr))\n",
    "        xDataTS = np.hstack((xDataTS,monData))\n",
    "    \n",
    "    return xDataTS, outputLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking data from:/home/jcharla/LiporLab/Data/M122_00_M118_50_33_40_36_40/MMSI_1004_TRAIN_INTP/\n",
      "128\n",
      "7\n",
      "Taking data from:/home/jcharla/LiporLab/Data/M122_00_M118_50_33_40_36_40/MMSI_1024_TRAIN_INTP/\n",
      "210\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#LAT, LON, SOG, COG, Month (12), Type (2)\n",
    "tSCol = 18\n",
    "tSCol = tSCol * prevTS\n",
    "xDataTS = np.zeros((0,tSCol))\n",
    "yData = np.zeros((0,4))\n",
    "for vesselDataSource in vesselDataSources:\n",
    "    print(\"Taking data from:%s\"%vesselDataSource.srcDir)\n",
    "    mMSIListFile = vesselDataSource.srcDir + 'TrainTrajCount.txt'\n",
    "    mMSIList = [line.rstrip('\\n') for line in open(mMSIListFile)]\n",
    "    for traj in mMSIList[0:1]:\n",
    "        numTraj = traj.split(\"_\")[-1]\n",
    "        trajAffix = traj[0:-len(numTraj)]\n",
    "        for num in range(int(numTraj)):\n",
    "            trajName = trajAffix + str(num) +'.csv'\n",
    "            rawData = get_traj_data(trajName)\n",
    "            xTSTemp, yTemp = convert_seq_to_x_y(rawData, vesselDataSource.type, prevTS)\n",
    "            xDataTS = np.vstack((xDataTS,xTSTemp.copy()))\n",
    "            yData = np.vstack((yData,yTemp.copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338, 216)\n",
      "(338, 4)\n"
     ]
    }
   ],
   "source": [
    "print(xDataTS.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTSToStore = modelDir + \"XDataTS.npy\"\n",
    "yToStore = modelDir + \"YData.npy\"\n",
    "np.save(xTSToStore, xDataTS)\n",
    "np.save(yToStore, yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTSFeature = 18\n",
    "xTSToStore = modelDir + \"XDataTS.npy\"\n",
    "yToStore = modelDir + \"YData.npy\"\n",
    "\n",
    "xDataTS = np.load(xTSToStore, allow_pickle=True)\n",
    "yData = np.load(yToStore, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338, 216)\n",
      "(338, 4)\n"
     ]
    }
   ],
   "source": [
    "print(xDataTS.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.6 17.4\n"
     ]
    }
   ],
   "source": [
    "sogFirstCol = 2\n",
    "nextSOGCol = sogFirstCol\n",
    "sOGMin = np.min(xDataTS[:,nextSOGCol])\n",
    "sOGMax = np.max(xDataTS[:,nextSOGCol])\n",
    "while(nextSOGCol < xDataTS.shape[1]):\n",
    "    tempMinSOG = np.min(xDataTS[:,nextSOGCol])\n",
    "    tempMaxSOG = np.max(xDataTS[:,nextSOGCol])\n",
    "    if(tempMinSOG < sOGMin):\n",
    "        sOGMin = tempMinSOG\n",
    "        \n",
    "    if(tempMaxSOG > sOGMax):\n",
    "        sOGMax = tempMaxSOG\n",
    "        \n",
    "    nextSOGCol = nextSOGCol + numTSFeature\n",
    "print(sOGMin, sOGMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-151.30285714285714 -79.8855072463768\n"
     ]
    }
   ],
   "source": [
    "cogFirstCol = 3\n",
    "nextCOGCol = cogFirstCol\n",
    "cOGMin = np.min(xDataTS[:,nextCOGCol])\n",
    "cOGMax = np.max(xDataTS[:,nextCOGCol])\n",
    "while(nextCOGCol < xDataTS.shape[1]):\n",
    "    tempMinCOG = np.min(xDataTS[:,nextCOGCol])\n",
    "    tempMaxCOG = np.max(xDataTS[:,nextCOGCol])\n",
    "    if(tempMinCOG < cOGMin):\n",
    "        cOGMin = tempMinCOG\n",
    "        \n",
    "    if(tempMaxCOG > cOGMax):\n",
    "        cOGMax = tempMaxCOG\n",
    "        \n",
    "    nextCOGCol = nextCOGCol + numTSFeature\n",
    "print(cOGMin, cOGMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33.71813 -118.50098 11.7 -110.6 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 33.7260747826087 -118.51812347826088 11.72608695652174\n",
      " -111.8608695652174 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 33.733780163934426 -118.53519327868851 11.6 -112.23934426229508 1.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 33.741399444444454\n",
      " -118.55233611111109 11.797222222222222 -110.6 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 1.0 0.0 0.0 0.0 0.0 0.0 0.0 33.74914500000001 -118.56953585714287\n",
      " 11.727142857142857 -110.32857142857141 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 33.75685285714286 -118.58660857142857\n",
      " 11.655714285714286 -112.04285714285713 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 33.764920000000004 -118.60358 11.6 -109.6 1.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 33.77299855072464\n",
      " -118.62031913043478 11.657971014492755 -110.17971014492753 1.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 33.78121260869565\n",
      " -118.63726782608695 11.730434782608695 -108.6 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 1.0 0.0 0.0 0.0 0.0 0.0 0.0 33.789659726027395 -118.65390493150684\n",
      " 11.663013698630134 -106.9150684931507 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 33.79827 -118.67050585714287 11.887142857142855\n",
      " -105.72857142857143 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 33.80706657142857 -118.687106 11.7 -106.6 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 1.0 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      "[33.8157323943662 -118.70347816901408 11.7 -107.6]\n",
      "[33.7260747826087 -118.51812347826088 11.72608695652174 -111.8608695652174\n",
      " 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 33.733780163934426 -118.53519327868851 11.6 -112.23934426229508 1.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 33.741399444444454\n",
      " -118.55233611111109 11.797222222222222 -110.6 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 1.0 0.0 0.0 0.0 0.0 0.0 0.0 33.74914500000001 -118.56953585714287\n",
      " 11.727142857142857 -110.32857142857141 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 33.75685285714286 -118.58660857142857\n",
      " 11.655714285714286 -112.04285714285713 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 33.764920000000004 -118.60358 11.6 -109.6 1.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 33.77299855072464\n",
      " -118.62031913043478 11.657971014492755 -110.17971014492753 1.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 33.78121260869565\n",
      " -118.63726782608695 11.730434782608695 -108.6 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 1.0 0.0 0.0 0.0 0.0 0.0 0.0 33.789659726027395 -118.65390493150684\n",
      " 11.663013698630134 -106.9150684931507 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 33.79827 -118.67050585714287 11.887142857142855\n",
      " -105.72857142857143 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 33.80706657142857 -118.687106 11.7 -106.6 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 1.0 0.0 0.0 0.0 0.0 0.0 0.0 33.8157323943662 -118.70347816901408 11.7\n",
      " -107.6 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      "[33.82465652173913 -118.72002869565216 11.91304347826087\n",
      " -106.7304347826087]\n"
     ]
    }
   ],
   "source": [
    "print(xDataTS[0,:])\n",
    "print(yData[0,:])\n",
    "print(xDataTS[1,:])\n",
    "print(yData[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise the data\n",
    "xDataTSNorm = xDataTS.copy()\n",
    "colAccess = 0\n",
    "for prevTime in range(prevTS):\n",
    "    xDataTSNorm[:,(colAccess) + 0] = (xDataTS[:,(colAccess) + 0] - latMin)/(latMax - latMin)\n",
    "    xDataTSNorm[:,(colAccess) + 1] = (xDataTS[:,(colAccess) + 1] - lonMin)/(lonMax - lonMin)\n",
    "    xDataTSNorm[:,(colAccess) + 2] = (xDataTS[:,(colAccess) + 2] - sOGMin)/(sOGMax - sOGMin)\n",
    "    xDataTSNorm[:,(colAccess) + 3] = (xDataTS[:,(colAccess) + 3] - cOGMin)/(cOGMax - cOGMin)\n",
    "    colAccess = colAccess + numTSFeature\n",
    "\n",
    "    \n",
    "xDataTSNorm = np.reshape(xDataTSNorm,(xDataTSNorm.shape[0], prevTS, numTSFeature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise the output data as well\n",
    "yLonLatData_0 = (yData[:,0] - latMin)/(latMax - latMin)\n",
    "yLonLatData_0 = np.reshape(yLonLatData_0,(yLonLatData_0.shape[0],1))\n",
    "yLonLatData_1 = (yData[:,1] - lonMin)/(lonMax - lonMin)\n",
    "yLonLatData_1 = np.reshape(yLonLatData_1,(yLonLatData_1.shape[0],1))\n",
    "yLonLatData_2 = (yData[:,2] - sOGMin)/(sOGMax - sOGMin)\n",
    "yLonLatData_2 = np.reshape(yLonLatData_2,(yLonLatData_2.shape[0],1))\n",
    "yLonLatData_3 = (yData[:,3] - cOGMin)/(cOGMax - cOGMin)\n",
    "yLonLatData_3 = np.reshape(yLonLatData_3,(yLonLatData_3.shape[0],1))\n",
    "yDataNorm = np.hstack((yLonLatData_0,yLonLatData_1,yLonLatData_2,yLonLatData_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0808 13:47:10.263092 139885656610624 deprecation_wrapper.py:119] From /home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0808 13:47:10.263851 139885656610624 deprecation_wrapper.py:119] From /home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0808 13:47:10.264376 139885656610624 deprecation_wrapper.py:119] From /home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0808 13:47:11.533099 139885656610624 deprecation_wrapper.py:119] From /home/jcharla/anaconda3/envs/conan_pred_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lonLatTS = Input(shape=(prevTS,numTSFeature))\n",
    "# hidden1 = LSTM(50, return_sequences= True)(lonLatTS)\n",
    "# hidden21 = LSTM(50, dropout=0.5, recurrent_dropout=0.5)(hidden1)\n",
    "\n",
    "# hidden22 = LSTM(50, dropout=0.5, recurrent_dropout=0.5)(hidden1)\n",
    "# hidden23 = LSTM(50, dropout=0.5, recurrent_dropout=0.5)(hidden1)\n",
    "# hidden24 = LSTM(50, dropout=0.5, recurrent_dropout=0.5)(hidden1)\n",
    "\n",
    "# lonDense1Drop = Dropout(0.5)(hidden21)\n",
    "# latDense1Drop = Dropout(0.5)(hidden22)\n",
    "# sOGDense1Drop = Dropout(0.5)(hidden23)\n",
    "# cOGDense1Drop = Dropout(0.5)(hidden24)\n",
    "\n",
    "# lonDense1 = Dense(150, activation='relu')(lonDense1Drop)\n",
    "# latDense1 = Dense(150, activation='relu')(latDense1Drop)\n",
    "# sOGDense1 = Dense(150, activation='relu')(sOGDense1Drop)\n",
    "# cOGDense1 = Dense(150, activation='relu')(cOGDense1Drop)\n",
    "\n",
    "# lonDense2Drop = Dropout(0.5)(lonDense1)\n",
    "# latDense2Drop = Dropout(0.5)(latDense1)\n",
    "# sOGDense2Drop = Dropout(0.5)(sOGDense1)\n",
    "# cOGDense2Drop = Dropout(0.5)(cOGDense1)\n",
    "\n",
    "# lonDense2 = Dense(150, activation='relu')(lonDense2Drop)\n",
    "# latDense2 = Dense(150, activation='relu')(latDense2Drop)\n",
    "# sOGDense2 = Dense(150, activation='relu')(sOGDense2Drop)\n",
    "# cOGDense2 = Dense(150, activation='relu')(cOGDense2Drop)\n",
    "\n",
    "# lonOp = Dense(1, activation='linear')(lonDense2)\n",
    "# latOp = Dense(1, activation='linear')(latDense2)\n",
    "# sOGOp = Dense(1, activation='linear')(sOGDense2)\n",
    "# cOGOp = Dense(1, activation='linear')(cOGDense2)\n",
    "\n",
    "# modelMultiTask = Model(inputs=lonLatTS, outputs=[lonOp,latOp,sOGOp,cOGOp])\n",
    "\n",
    "lonLatTS = Input(shape=(prevTS,numTSFeature))\n",
    "hidden1 = LSTM(50, return_sequences= True)(lonLatTS)\n",
    "# hidden21 = LSTM(50)(hidden1)\n",
    "# hidden22 = LSTM(50)(hidden1)\n",
    "# hidden23 = LSTM(50)(hidden1)\n",
    "# hidden24 = LSTM(50)(hidden1)\n",
    "hidden2 = LSTM(50)(hidden1)\n",
    "\n",
    "lonDense1 = Dense(150, activation='relu')(hidden2)\n",
    "latDense1 = Dense(150, activation='relu')(hidden2)\n",
    "sOGDense1 = Dense(150, activation='relu')(hidden2)\n",
    "cOGDense1 = Dense(150, activation='relu')(hidden2)\n",
    "\n",
    "lonDense2 = Dense(150, activation='relu')(lonDense1)\n",
    "latDense2 = Dense(150, activation='relu')(latDense1)\n",
    "sOGDense2 = Dense(150, activation='relu')(sOGDense1)\n",
    "cOGDense2 = Dense(150, activation='relu')(cOGDense1)\n",
    "\n",
    "lonOp = Dense(1, activation='linear')(lonDense2)\n",
    "latOp = Dense(1, activation='linear')(latDense2)\n",
    "sOGOp = Dense(1, activation='linear')(sOGDense2)\n",
    "cOGOp = Dense(1, activation='linear')(cOGDense2)\n",
    "\n",
    "modelMultiTask = Model(inputs=lonLatTS, outputs=[lonOp,latOp,sOGOp,cOGOp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMultiTask.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelMultiTask.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# modelMultiTaskHist = modelMultiTask.fit(xDataTSNorm, [yDataNorm[:,0],yDataNorm[:,1],yDataNorm[:,2],yDataNorm[:,3]], epochs=50, batch_size = 1024 , verbose = 2)\n",
    "modelMultiTaskHist = modelMultiTask.fit(xDataTSNorm, [yDataNorm[:,0],yDataNorm[:,1],yDataNorm[:,2],yDataNorm[:,3]], validation_data = [xDataTSTestNorm,[yDataTestNorm[:,0],yDataTestNorm[:,1],yDataTestNorm[:,2],yDataTestNorm[:,3]]], epochs=1000, batch_size = 1024 , verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(modelMultiTaskHist.history['loss'])\n",
    "plt.plot(modelMultiTaskHist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMultiTaskDir = dataDir + \"Model_MULTI_TASK_1000_MSE_WODROP_RANDOM_SAMPLE.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMultiTask.save(modelMultiTaskDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('trainWODROPRANDOMSAMPLE', 'wb') as file_pi:\n",
    "    pickle.dump(modelMultiTaskHist.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "modelMultiTask = load_model(modelMultiTaskDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lon_lat(arr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #subtract the minimum \n",
    "    #and divide by range\n",
    "    ret0 = (arr[:,0] - lonMin)/(lonMax - lonMin)\n",
    "    ret0 = np.reshape(ret0, (ret0.shape[0],1))\n",
    "    ret1 = (arr[:,1] - latMin)/(latMax - latMin)\n",
    "    ret1 = np.reshape(ret1, (ret1.shape[0],1))\n",
    "    ret = np.hstack((ret0, ret1))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes 2 time stamps of LON and LAT\n",
    "#normalises them and \n",
    "#and makes the prediction\n",
    "#de normalize the output\n",
    "#and return the values\n",
    "#prevTraj numpy array of 1x2\n",
    "#currTraj numpy array of 1x2\n",
    "# def compute_30_min_pred(prevTraj, currTraj, typeVessel):\n",
    "def compute_30_min_pred(prevTraj, lenVal, yearVal, monVal, destLon, destLat, vType, sOGVal, cOGVal):\n",
    "    \"\"\"\n",
    "    Compute prediction for 30 minutes.\n",
    "    \n",
    "    takes LON and LAT of previous time stamps\n",
    "    normalises them\n",
    "    and makes prediction\n",
    "    and returns denormalised LON and LAT values\n",
    "    \"\"\"\n",
    "    #this will be    \n",
    "    prevTimeStamp = prevTraj.shape[0]\n",
    "    prevTrajNorm = normalize_lon_lat(prevTraj)\n",
    "        \n",
    "    monData = np.zeros((prevTimeStamp, 12))\n",
    "    monData[:,(12-monVal)] = 1\n",
    "    \n",
    "    destArr = np.zeros((prevTimeStamp,2))\n",
    "    destArr[:,0] = destLon\n",
    "    destArr[:,1] = destLat\n",
    "\n",
    "    destArr[:,0] = (destArr[:,0] - lonMin)/(lonMax - lonMin)\n",
    "    destArr[:,1] = (destArr[:,1] - latMin)/(latMax - latMin)\n",
    "    \n",
    "    typeData = np.zeros((prevTimeStamp,2))\n",
    "    typeData[:,vType] = 1\n",
    "    \n",
    "    xTSNorm = np.zeros((prevTimeStamp,0))\n",
    "    xFNorm = np.zeros((1,0))\n",
    "    \n",
    "    sOGValNorm = (sOGVal[:,:] - sOGMin)/(sOGMax - sOGMin)\n",
    "    cOGValNorm = (cOGVal[:,:] - cOGMin)/(cOGMax - cOGMin)\n",
    "    \n",
    "    xTSNorm = np.hstack((xTSNorm,prevTrajNorm))\n",
    "    xTSNorm = np.hstack((xTSNorm,sOGValNorm))\n",
    "    xTSNorm = np.hstack((xTSNorm,cOGValNorm))\n",
    "    xTSNorm = np.hstack((xTSNorm,typeData))\n",
    "    xTSNorm = np.hstack((xTSNorm,monData))\n",
    "    xTSNorm = np.hstack((xTSNorm,destArr))\n",
    "\n",
    "#     xFNorm = np.hstack((xFNorm,typeData))\n",
    "#     xFNorm = np.hstack((xFNorm,monData))\n",
    "#     xFNorm = np.hstack((xFNorm,destArr))\n",
    "    \n",
    "    xTSNorm = np.reshape(xTSNorm,(1,xTSNorm.shape[0],xTSNorm.shape[1]))\n",
    "    predLon, predLat, predSOG, predCOG = modelMultiTask.predict(xTSNorm)\n",
    "    \n",
    "    #after prediction de normalise it\n",
    "    predLonScaled = (predLon * (lonMax - lonMin)) + lonMin\n",
    "    predLatScaled = (predLat * (latMax - latMin)) + latMin\n",
    "    predSOGScaled = (predSOG * (sOGMax - sOGMin)) + sOGMin\n",
    "    predCOGScaled = (predCOG * (cOGMax - cOGMin)) + cOGMin\n",
    "    return predLonScaled, predLatScaled, predSOGScaled, predCOGScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_30_min_pred(prevTraj, lenVal, yearVal, monVal, destLon, destLat, vType, sOGVal, cOGVal, n = 1):\n",
    "    \"\"\"\n",
    "    Compute prediction for n*30 minutes.\n",
    "    \n",
    "    calls compute_30_min_pred\n",
    "    according to value of n\n",
    "    default value of n = 1\n",
    "    \"\"\"\n",
    "    #temp vaariables to store the previous trajectory\n",
    "    prevTimeStamp = prevTraj.shape[0]\n",
    "    \n",
    "    firstLoc = prevTraj.copy()\n",
    "    firstSOG = sOGVal.copy()\n",
    "    firstCOG = cOGVal.copy()\n",
    "    \n",
    "    #return value will be numpy array of nx2\n",
    "    ret = np.zeros((0,4))\n",
    "    for i in range(n):\n",
    "        #make prediction using 30 min pred\n",
    "        predLon, predLat, predSOG, predCOG = compute_30_min_pred(firstLoc, lenVal, yearVal, monVal, destLon, destLat, vType, firstSOG, firstCOG)\n",
    "        #append them in ret\n",
    "        ret = np.vstack((ret,np.array([[predLon.flatten()[0] \\\n",
    "                                        ,predLat.flatten()[0] \\\n",
    "                                        ,predSOG.flatten()[0] \\\n",
    "                                        ,predCOG.flatten()[0]]])))\n",
    "        \n",
    "        #update firstLoc \n",
    "        #for next  iteration\n",
    "        firstLoc = firstLoc[1:,:].copy()\n",
    "        firstSOG = firstSOG[1:,:].copy()\n",
    "        firstCOG = firstCOG[1:,:].copy()\n",
    "        firstLoc = np.vstack((firstLoc,np.array([[predLon.flatten()[0],predLat.flatten()[0]]])))\n",
    "        firstSOG = np.vstack((firstSOG,np.array([[predSOG.flatten()[0]]])))\n",
    "        firstCOG = np.vstack((firstCOG,np.array([[predCOG.flatten()[0]]])))\n",
    "#     return predArr\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_for_traj(srcDir, num, vesselType):\n",
    "    \"\"\"\n",
    "    Compute error for the entire trajectory.\n",
    "    \n",
    "    Returns list of error for predictions of upto \n",
    "    4 hours i.e. 240 minutes.\n",
    "    \n",
    "    Parameters:\n",
    "        num (int): number of vessel trajectory \n",
    "                   whose prediction errors to be computed.\n",
    "    Returns:\n",
    "        errorVal (list of floats): list of errors in KM for each\n",
    "                                   30 min predictions.\n",
    "    \"\"\"    \n",
    "    errorVal = []\n",
    "    \n",
    "    vesselTraj,lenData,yearData,monData,destLon,destLat,vType,sOGData,cOGData = get_traj_lon_lat_data_with_len(srcDir, num, vesselType)\n",
    "\n",
    "    if(vesselTraj.shape[0] < (prevTS + 1)):\n",
    "        return errorVal\n",
    "        \n",
    "        \n",
    "    predVesselTraj = compute_n_30_min_pred(np.reshape(vesselTraj[0:prevTS,:], (prevTS,2)), lenData, yearData, monData, destLon, destLat, vType \\\n",
    "                                           , np.reshape(sOGData[0:prevTS,:], (prevTS,1))\\\n",
    "                                           , np.reshape(cOGData[0:prevTS,:], (prevTS,1))\\\n",
    "                                           ,n = 8)\n",
    "    predRange = vesselTraj.shape[0] - prevTS\n",
    "\n",
    "    #FIXME 8 can come from some variable too\n",
    "    #for 8 consecutive predictions\n",
    "    if(predRange > 8):\n",
    "        predRange = 8\n",
    "\n",
    "    #use trajectories which has atleast one\n",
    "    #value to predict\n",
    "    if(predRange >= 0):\n",
    "        for pred in range(predRange):    \n",
    "            trueLon, trueLat = vesselTraj[(prevTS + pred),0], vesselTraj[(prevTS + pred),1]\n",
    "            predLon, predLat = predVesselTraj[pred,0], predVesselTraj[pred,1]\n",
    "    #         print(trueLon, trueLat, '-', predLon, predLat)\n",
    "            errorVal.append(gC.compute_distance(trueLon, trueLat, predLon, predLat))\n",
    "    return errorVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "trainDataWholeErrors_1004 = []\n",
    "# for traj in range(trainTrajNum):\n",
    "for vesselDataSource in vesselDataSources[0:1]:\n",
    "    print(\"Taking data from:%s\"%vesselDataSource.srcDir)\n",
    "    for traj in vesselDataSource.trainList:\n",
    "        trainDataWholeErrors_1004.append(get_error_for_traj(vesselDataSource.srcDir,traj,vesselDataSource.type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "trainWholeErr_1004_30 = []\n",
    "trainWholeErr_1004_60 = []\n",
    "trainWholeErr_1004_90 = []\n",
    "trainWholeErr_1004_120 = []\n",
    "trainWholeErr_1004_150 = []\n",
    "trainWholeErr_1004_180 = []\n",
    "trainWholeErr_1004_210 = []\n",
    "trainWholeErr_1004_240 = []\n",
    "\n",
    "trainWholeErr_1004_n_30 = [trainWholeErr_1004_30 \\\n",
    ", trainWholeErr_1004_60 \\\n",
    ", trainWholeErr_1004_90 \\\n",
    ", trainWholeErr_1004_120 \\\n",
    ", trainWholeErr_1004_150 \\\n",
    ", trainWholeErr_1004_180 \\\n",
    ", trainWholeErr_1004_210 \\\n",
    ", trainWholeErr_1004_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in trainDataWholeErrors_1004:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            trainWholeErr_1004_n_30[n_30].append(trajErr[n_30])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
