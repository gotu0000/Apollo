{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import configparser\n",
    "sys.path.insert(0, 'Utils/')\n",
    "\n",
    "from AISDataManager import AISDataManager\n",
    "import SimpleUtils as sU\n",
    "import Constants as c\n",
    "import TimeUtils as timeUtils\n",
    "import HMUtils as hMUtil\n",
    "import GeoCompute as gC\n",
    "\n",
    "aISDM = AISDataManager()\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('DefaultConfig.INI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonMin = (float)(config['LSTM_MULTI_TASK_DEST_TRAIN']['LON_MIN'])\n",
    "lonMax = (float)(config['LSTM_MULTI_TASK_DEST_TRAIN']['LON_MAX'])\n",
    "\n",
    "latMin = (float)(config['LSTM_MULTI_TASK_DEST_TRAIN']['LAT_MIN'])\n",
    "latMax = (float)(config['LSTM_MULTI_TASK_DEST_TRAIN']['LAT_MAX'])\n",
    "\n",
    "print(lonMin,latMin)\n",
    "print(lonMax,latMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VesselTypeSource:\n",
    "    \"\"\"\n",
    "    The VesselTypeSource object contains information. \n",
    "    which is useful for the training data\n",
    "    \"\"\"\n",
    "    def __init__(self, srcDir, typeVes):\n",
    "        self.srcDir = srcDir\n",
    "        self.type = typeVes\n",
    "        self.trainList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add more sources here based on requirement\n",
    "sourceDir1 = config['LSTM_MULTI_TASK_DEST_TRAIN']['SRC_DIR_1']\n",
    "sourceDir1Test = config['LSTM_MULTI_TASK_DEST_TRAIN']['SRC_DIR_TEST_1']\n",
    "sourceDir2 = config['LSTM_MULTI_TASK_DEST_TRAIN']['SRC_DIR_2']\n",
    "sourceDir2Test = config['LSTM_MULTI_TASK_DEST_TRAIN']['SRC_DIR_TEST_2']\n",
    "\n",
    "modelDir = config['LSTM_MULTI_TASK_DEST_TRAIN']['MODEL_DIR']\n",
    "modelTS = int(config['LSTM_MULTI_TASK_DEST_TRAIN']['MODEL_TS'])\n",
    "prevTS = int(60/modelTS)+1\n",
    "#in minutes\n",
    "dataMinimumUnit = 5\n",
    "print(sourceDir1)\n",
    "print(sourceDir2)\n",
    "print(modelDir)\n",
    "print(prevTS)\n",
    "tsSkip = int(modelTS/dataMinimumUnit) - 1\n",
    "print(tsSkip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vesselSource1 = VesselTypeSource(sourceDir1,0)\n",
    "vesselSource2 = VesselTypeSource(sourceDir2,1)\n",
    "\n",
    "vesselSource1Test = VesselTypeSource(sourceDir1Test,0)\n",
    "vesselSource2Test = VesselTypeSource(sourceDir2Test,1)\n",
    "\n",
    "vesselDataSources = [vesselSource1, vesselSource2]\n",
    "vesselDataSourcesTest = [vesselSource1Test, vesselSource2Test]\n",
    "print(vesselDataSources[0])\n",
    "print(vesselDataSources[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_traj_data(fileName):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #TODO:uncomment/comment next two lines\n",
    "#     sourceDF,_ = aISDM.load_data_from_csv(fileName)\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(fileName.replace(\"jcharla\",\"jagir\"))\n",
    "    return sourceDF.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seq_to_x_y(seq, vType, prevTimeStamp):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #-prevTimeStamp is is to take care of boundary condition\n",
    "    #since we are considering prevTimeStamp time stamps for the input data\n",
    "    xNumRows = seq[:-(prevTimeStamp),:].shape[0]\n",
    "    print(xNumRows)\n",
    "    dateStr = seq[0][0]\n",
    "    monF = int(dateStr.split(\"-\")[1])\n",
    "    print(monF)\n",
    "    monData = np.zeros((xNumRows, 12))\n",
    "    monData[:,(12-monF)] = 1\n",
    "    \n",
    "    #TODO: change 2 for multiple types\n",
    "    typeArr = np.zeros((xNumRows,2))\n",
    "    typeArr[:,vType] = 1\n",
    "    \n",
    "    lonLatColList = []\n",
    "    for start in range(prevTimeStamp):\n",
    "        lonLatColList.append(seq[start:(-prevTimeStamp+start),1:].copy())\n",
    "        \n",
    "    outputLabelLonLat = seq[prevTimeStamp:,[1,2,3,4]].copy()\n",
    "    \n",
    "    outputLabel = (outputLabelLonLat)\n",
    "    \n",
    "    xDataTS = np.zeros((xNumRows,0))\n",
    "    \n",
    "    for tS in range(prevTimeStamp):\n",
    "        xDataTS = np.hstack((xDataTS,lonLatColList[tS]))\n",
    "        xDataTS = np.hstack((xDataTS,typeArr))\n",
    "        xDataTS = np.hstack((xDataTS,monData))\n",
    "    \n",
    "    return xDataTS, outputLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#prepare data in x, y formate\n",
    "#this is memory intensive task may take a time\n",
    "#LAT, LON, SOG, COG, LAT_DEST, LON_DEST, Month (12), Type (2)\n",
    "tSCol = 20\n",
    "tSCol = tSCol * prevTS\n",
    "xDataTS = np.zeros((0,tSCol))\n",
    "yData = np.zeros((0,4))\n",
    "# for vesselDataSource in vesselDataSources[0:1]:\n",
    "for vesselDataSource in vesselDataSources[0:]:\n",
    "    print(\"Taking data from:%s\"%vesselDataSource.srcDir)\n",
    "    mMSIListFile = vesselDataSource.srcDir + 'TrainTrajCount.txt'\n",
    "    mMSIList = [line.rstrip('\\n') for line in open(mMSIListFile)]\n",
    "#     for traj in mMSIList[0:1]:\n",
    "    for traj in mMSIList[0:]:\n",
    "        numTraj = traj.split(\"_\")[-1]\n",
    "        trajAffix = traj[0:-len(numTraj)]\n",
    "        for num in range(int(numTraj)):\n",
    "            trajName = trajAffix + str(num) +'.csv'\n",
    "            rawData = get_traj_data(trajName)\n",
    "#             print(rawData.shape)\n",
    "            for trajSkip in range(tsSkip+1):\n",
    "                selectIDX = np.arange(trajSkip, rawData.shape[0], (tsSkip+1))\n",
    "#                 print(selectIDX)\n",
    "                selectedData = rawData[selectIDX,:]\n",
    "#                 print(selectedData)\n",
    "                xTSTemp, yTemp = convert_seq_to_x_y(selectedData, vesselDataSource.type, prevTS)\n",
    "#                 print(xTSTemp)\n",
    "                xDataTS = np.vstack((xDataTS,xTSTemp.copy()))\n",
    "                yData = np.vstack((yData,yTemp.copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTS)\n",
    "print(yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTS.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prepared data\n",
    "xTSToStore = modelDir + \"XDataTS.npy\"\n",
    "yToStore = modelDir + \"YData.npy\"\n",
    "np.save(xTSToStore, xDataTS)\n",
    "np.save(yToStore, yData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data can be loaded directly, if it has been prepared earlier and saved\n",
    "numTSFeature = 20\n",
    "xTSToStore = modelDir + \"XDataTS.npy\"\n",
    "yToStore = modelDir + \"YData.npy\"\n",
    "\n",
    "xDataTS = np.load(xTSToStore, allow_pickle=True)\n",
    "yData = np.load(yToStore, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTS.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Min and max for SOG\n",
    "sogFirstCol = 2\n",
    "nextSOGCol = sogFirstCol\n",
    "sOGMin = np.min(xDataTS[:,nextSOGCol])\n",
    "sOGMax = np.max(xDataTS[:,nextSOGCol])\n",
    "while(nextSOGCol < xDataTS.shape[1]):\n",
    "    tempMinSOG = np.min(xDataTS[:,nextSOGCol])\n",
    "    tempMaxSOG = np.max(xDataTS[:,nextSOGCol])\n",
    "    if(tempMinSOG < sOGMin):\n",
    "        sOGMin = tempMinSOG\n",
    "        \n",
    "    if(tempMaxSOG > sOGMax):\n",
    "        sOGMax = tempMaxSOG\n",
    "        \n",
    "    nextSOGCol = nextSOGCol + numTSFeature\n",
    "print(sOGMin, sOGMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Min and max for COG\n",
    "cogFirstCol = 3\n",
    "nextCOGCol = cogFirstCol\n",
    "cOGMin = np.min(xDataTS[:,nextCOGCol])\n",
    "cOGMax = np.max(xDataTS[:,nextCOGCol])\n",
    "while(nextCOGCol < xDataTS.shape[1]):\n",
    "    tempMinCOG = np.min(xDataTS[:,nextCOGCol])\n",
    "    tempMaxCOG = np.max(xDataTS[:,nextCOGCol])\n",
    "    if(tempMinCOG < cOGMin):\n",
    "        cOGMin = tempMinCOG\n",
    "        \n",
    "    if(tempMaxCOG > cOGMax):\n",
    "        cOGMax = tempMaxCOG\n",
    "        \n",
    "    nextCOGCol = nextCOGCol + numTSFeature\n",
    "print(cOGMin, cOGMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTS[0,:])\n",
    "print(yData[0,:])\n",
    "print(xDataTS[1,:])\n",
    "print(yData[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise the data\n",
    "xDataTSNorm = xDataTS.copy()\n",
    "colAccess = 0\n",
    "for prevTime in range(prevTS):\n",
    "    xDataTSNorm[:,(colAccess) + 0] = (xDataTS[:,(colAccess) + 0] - latMin)/(latMax - latMin)\n",
    "    xDataTSNorm[:,(colAccess) + 1] = (xDataTS[:,(colAccess) + 1] - lonMin)/(lonMax - lonMin)\n",
    "    xDataTSNorm[:,(colAccess) + 2] = (xDataTS[:,(colAccess) + 2] - sOGMin)/(sOGMax - sOGMin)\n",
    "    xDataTSNorm[:,(colAccess) + 3] = (xDataTS[:,(colAccess) + 3] - cOGMin)/(cOGMax - cOGMin)\n",
    "    xDataTSNorm[:,(colAccess) + 4] = (xDataTS[:,(colAccess) + 4] - latMin)/(latMax - latMin)\n",
    "    xDataTSNorm[:,(colAccess) + 5] = (xDataTS[:,(colAccess) + 5] - lonMin)/(lonMax - lonMin)\n",
    "    colAccess = colAccess + numTSFeature\n",
    "\n",
    "    \n",
    "xDataTSNorm = np.reshape(xDataTSNorm,(xDataTSNorm.shape[0], prevTS, numTSFeature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise the output data as well\n",
    "yLonLatData_0 = (yData[:,0] - latMin)/(latMax - latMin)\n",
    "yLonLatData_0 = np.reshape(yLonLatData_0,(yLonLatData_0.shape[0],1))\n",
    "yLonLatData_1 = (yData[:,1] - lonMin)/(lonMax - lonMin)\n",
    "yLonLatData_1 = np.reshape(yLonLatData_1,(yLonLatData_1.shape[0],1))\n",
    "yLonLatData_2 = (yData[:,2] - sOGMin)/(sOGMax - sOGMin)\n",
    "yLonLatData_2 = np.reshape(yLonLatData_2,(yLonLatData_2.shape[0],1))\n",
    "yLonLatData_3 = (yData[:,3] - cOGMin)/(cOGMax - cOGMin)\n",
    "yLonLatData_3 = np.reshape(yLonLatData_3,(yLonLatData_3.shape[0],1))\n",
    "yDataNorm = np.hstack((yLonLatData_0,yLonLatData_1,yLonLatData_2,yLonLatData_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "lonLatTS = Input(shape=(prevTS,numTSFeature))\n",
    "hidden1 = LSTM(50, return_sequences= True)(lonLatTS)\n",
    "# hidden21 = LSTM(50)(hidden1)\n",
    "# hidden22 = LSTM(50)(hidden1)\n",
    "# hidden23 = LSTM(50)(hidden1)\n",
    "# hidden24 = LSTM(50)(hidden1)\n",
    "hidden2 = LSTM(50)(hidden1)\n",
    "\n",
    "latDense1 = Dense(150, activation='relu')(hidden2)\n",
    "lonDense1 = Dense(150, activation='relu')(hidden2)\n",
    "sOGDense1 = Dense(150, activation='relu')(hidden2)\n",
    "cOGDense1 = Dense(150, activation='relu')(hidden2)\n",
    "\n",
    "latDense2 = Dense(150, activation='relu')(latDense1)\n",
    "lonDense2 = Dense(150, activation='relu')(lonDense1)\n",
    "sOGDense2 = Dense(150, activation='relu')(sOGDense1)\n",
    "cOGDense2 = Dense(150, activation='relu')(cOGDense1)\n",
    "\n",
    "latOp = Dense(1, activation='linear')(latDense2)\n",
    "lonOp = Dense(1, activation='linear')(lonDense2)\n",
    "sOGOp = Dense(1, activation='linear')(sOGDense2)\n",
    "cOGOp = Dense(1, activation='linear')(cOGDense2)\n",
    "\n",
    "modelMultiTask = Model(inputs=lonLatTS, outputs=[latOp,lonOp,sOGOp,cOGOp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMultiTask.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelMultiTask.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "modelMultiTaskHist = modelMultiTask.fit(xDataTSNorm, [yDataNorm[:,0],yDataNorm[:,1],yDataNorm[:,2],yDataNorm[:,3]], epochs=1000, batch_size = 1024 , verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(modelMultiTaskHist.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMultiTaskDir = modelDir + \"MODEL_MULTI_TASK_1000_MSE.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "modelMultiTask.save(modelMultiTaskDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load the already saved model\n",
    "from keras.models import load_model\n",
    "modelMultiTask = load_model(modelMultiTaskDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_data_for_pred(fileName, vType):\n",
    "    tempSourceDF,_ = aISDM.load_data_from_csv(fileName)\n",
    "    tempSourceDF = tempSourceDF.to_numpy()\n",
    "    tempMonth = int(tempSourceDF[0][0].split(\"-\")[1])\n",
    "    tempType = vType\n",
    "    selectIDX = np.arange(0, tempSourceDF.shape[0], (tsSkip+1))\n",
    "    tempSourceDF = tempSourceDF[selectIDX,:]\n",
    "    tempTraj = tempSourceDF[0:prevTS,1:]\n",
    "    return tempTraj, tempType, tempMonth\n",
    "\n",
    "tempTestFile = \"/home/jagir/LiporLab/Data/M122_00_M118_50_33_40_36_40/MMSI_1004_TRAIN_WDEST_INTP/355745000_0_0_0.csv\"\n",
    "tempTraj, tempType, tempMonth = get_traj_data_for_pred(tempTestFile, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tempTraj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_traj_data(arr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #subtract the minimum \n",
    "    #and divide by range\n",
    "    ret0 = (arr[:,0] - latMin)/(latMax - latMin)\n",
    "    ret0 = np.reshape(ret0, (ret0.shape[0],1))\n",
    "    ret1 = (arr[:,1] - lonMin)/(lonMax - lonMin)\n",
    "    ret1 = np.reshape(ret1, (ret1.shape[0],1))\n",
    "    ret2 = (arr[:,2] - sOGMin)/(sOGMax - sOGMin)\n",
    "    ret2 = np.reshape(ret2, (ret2.shape[0],1))\n",
    "    ret3 = (arr[:,3] - cOGMin)/(cOGMax - cOGMin)\n",
    "    ret3 = np.reshape(ret3, (ret3.shape[0],1))\n",
    "    ret4 = (arr[:,4] - latMin)/(latMax - latMin)\n",
    "    ret4 = np.reshape(ret4, (ret4.shape[0],1))\n",
    "    ret5 = (arr[:,5] - lonMin)/(lonMax - lonMin)\n",
    "    ret5 = np.reshape(ret5, (ret5.shape[0],1))\n",
    "    ret = np.hstack((ret0, ret1, ret2, ret3, ret4, ret5))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction(prevTraj, vType, monthOfTraj):\n",
    "    \"\"\"\n",
    "    computes prediction of a function\n",
    "    \n",
    "    Parameter:\n",
    "        prevTraj:(prevTS)x6 (np array of LAT, LON, SOG, COG, DEST_LAT, DEST_LON)\n",
    "        vType:type of vessel (0 or 1)\n",
    "        monthOfTraj: month of trajectory (1 to 12)\n",
    "    \"\"\"\n",
    "    prevTrajNorm = normalize_traj_data(prevTraj)\n",
    "    \n",
    "    monData = np.zeros((prevTS, 12))\n",
    "    monData[:,(12-monthOfTraj)] = 1\n",
    "    \n",
    "    typeData = np.zeros((prevTS,2))\n",
    "    typeData[:,vType] = 1\n",
    "    \n",
    "    xTSNorm = np.zeros((prevTS,0))\n",
    "    xTSNorm = np.hstack((xTSNorm,prevTrajNorm))\n",
    "    xTSNorm = np.hstack((xTSNorm,typeData))\n",
    "    xTSNorm = np.hstack((xTSNorm,monData))\n",
    "    \n",
    "    xTSNorm = np.reshape(xTSNorm,(1,xTSNorm.shape[0],xTSNorm.shape[1]))\n",
    "    predLat, predLon, predSOG, predCOG = modelMultiTask.predict(xTSNorm)\n",
    "    \n",
    "    #after prediction de normalise it\n",
    "    predLatScaled = (predLat * (latMax - latMin)) + latMin\n",
    "    predLonScaled = (predLon * (lonMax - lonMin)) + lonMin\n",
    "    predSOGScaled = (predSOG * (sOGMax - sOGMin)) + sOGMin\n",
    "    predCOGScaled = (predCOG * (cOGMax - cOGMin)) + cOGMin\n",
    "    return predLatScaled, predLonScaled, predSOGScaled, predCOGScaled\n",
    "    \n",
    "compute_prediction(tempTraj, tempType, tempMonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_unit_pred(prevTraj, vType, monVal, n = 1):\n",
    "    \"\"\"\n",
    "    Compute prediction for n units\n",
    "    \n",
    "    calls compute_prediction\n",
    "    according to value of n\n",
    "    default value of n = 1\n",
    "    \"\"\"\n",
    "    firstLoc = prevTraj.copy()\n",
    "    destLAT = prevTraj[0,4]\n",
    "    destLON = prevTraj[0,5]\n",
    "    \n",
    "    #return value will be numpy array of nx4\n",
    "    ret = np.zeros((0,4))\n",
    "    for i in range(n):\n",
    "        #make prediction\n",
    "        predLat, predLon, predSOG, predCOG = compute_prediction(firstLoc, vType, monVal)\n",
    "        #append them in ret\n",
    "        ret = np.vstack((ret,np.array([[predLat.flatten()[0] \\\n",
    "                                        ,predLon.flatten()[0] \\\n",
    "                                        ,predSOG.flatten()[0] \\\n",
    "                                        ,predCOG.flatten()[0]]])))\n",
    "        \n",
    "        #update firstLoc \n",
    "        #for next  iteration\n",
    "        firstLoc = firstLoc[1:,:].copy()\n",
    "        firstLoc = np.vstack((firstLoc,np.array([[predLat.flatten()[0] \\\n",
    "                          , predLon.flatten()[0] \\\n",
    "                          , predSOG.flatten()[0] \\\n",
    "                          , predCOG.flatten()[0] \\\n",
    "                          , destLAT \\\n",
    "                          , destLON]])))\n",
    "    return ret\n",
    "\n",
    "compute_n_unit_pred(tempTraj, tempType, tempMonth, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_for_traj(trajFile, vesselType):\n",
    "    \"\"\"\n",
    "    Compute error for the entire trajectory.\n",
    "    \n",
    "    Returns list of error for predictions of upto \n",
    "    4 hours i.e. 240 minutes.\n",
    "    \n",
    "    Parameters:\n",
    "        trajFile (str): file containing trajectory\n",
    "    Returns:\n",
    "        errorVal (list of floats): list of errors in KM for each\n",
    "                                   30 min predictions.\n",
    "    \"\"\"    \n",
    "    errorVal = []\n",
    "    \n",
    "    vesselTraj, vType, monData = get_traj_data_for_pred(trajFile, vesselType)        \n",
    "    numOfPredFor240 = int(240/modelTS)    \n",
    "    predVesselTraj = compute_n_unit_pred(vesselTraj, vType, monData ,n = numOfPredFor240)    \n",
    "    vesselActTraj = get_traj_data(trajFile)\n",
    "    minShapeOfActTraj = int(300/dataMinimumUnit)+1\n",
    "    \n",
    "    if(vesselActTraj.shape[0] < (minShapeOfActTraj)):\n",
    "        #it should not go into this condition\n",
    "        #Trajetories excluding these can be used as a benchmark dataset\n",
    "        print(\"Trajectory not used for error\")\n",
    "        return []\n",
    "    \n",
    "    selectIDX = np.arange(0, vesselActTraj.shape[0], (tsSkip+1))\n",
    "    vesselActTraj = vesselActTraj[selectIDX,:]\n",
    "    \n",
    "    for pred in range(predVesselTraj.shape[0]):\n",
    "        trueLat, trueLon = vesselActTraj[(prevTS + pred),1], vesselActTraj[(prevTS + pred),2]\n",
    "        predLat, predLon= predVesselTraj[pred,0], predVesselTraj[pred,1]\n",
    "        errorVal.append(gC.compute_distance(trueLon, trueLat, predLon, predLat))\n",
    "    \n",
    "    usefulPredStart = int(30/modelTS)-1\n",
    "    usefulPredDiff = int(30/modelTS)\n",
    "    errorRet = []\n",
    "    for i in range(usefulPredStart, len(errorVal), usefulPredDiff):\n",
    "        errorRet.append(errorVal[i])\n",
    "    return errorRet\n",
    "\n",
    "get_error_for_traj(tempTestFile, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute errors for all training trajectories\n",
    "trainDataWholeErrors = []\n",
    "# for vesselDataSource in vesselDataSources[0:1]:\n",
    "for vesselDataSource in vesselDataSources[0:]:\n",
    "    print(\"Taking data from:%s\"%vesselDataSource.srcDir)\n",
    "    mMSIListFile = vesselDataSource.srcDir + 'TrainTrajCount.txt'\n",
    "    mMSIList = [line.rstrip('\\n') for line in open(mMSIListFile)]\n",
    "#     for traj in mMSIList[0:1]:\n",
    "    for traj in mMSIList[0:]:\n",
    "        numTraj = traj.split(\"_\")[-1]\n",
    "        trajAffix = traj[0:-len(numTraj)]\n",
    "        for num in range(int(numTraj)):\n",
    "            trajName = trajAffix + str(num) +'.csv'\n",
    "            #TODO:.replace statement can be removed in the future\n",
    "            trainDataWholeErrors.append(get_error_for_traj(trajName.replace(\"jcharla\",\"jagir\"),vesselDataSource.type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErr_30 = []\n",
    "trainWholeErr_60 = []\n",
    "trainWholeErr_90 = []\n",
    "trainWholeErr_120 = []\n",
    "trainWholeErr_150 = []\n",
    "trainWholeErr_180 = []\n",
    "trainWholeErr_210 = []\n",
    "trainWholeErr_240 = []\n",
    "\n",
    "trainWholeErr_n_30 = [trainWholeErr_30 \\\n",
    ", trainWholeErr_60 \\\n",
    ", trainWholeErr_90 \\\n",
    ", trainWholeErr_120 \\\n",
    ", trainWholeErr_150 \\\n",
    ", trainWholeErr_180 \\\n",
    ", trainWholeErr_210 \\\n",
    ", trainWholeErr_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in trainDataWholeErrors:\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            trainWholeErr_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErr_30NP = np.array(trainWholeErr_30)\n",
    "trainWholeErr_60NP = np.array(trainWholeErr_60)\n",
    "trainWholeErr_90NP = np.array(trainWholeErr_90)\n",
    "trainWholeErr_120NP = np.array(trainWholeErr_120)\n",
    "trainWholeErr_150NP = np.array(trainWholeErr_150)\n",
    "trainWholeErr_180NP = np.array(trainWholeErr_180)\n",
    "trainWholeErr_210NP = np.array(trainWholeErr_210)\n",
    "trainWholeErr_240NP = np.array(trainWholeErr_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErrMean = [np.mean(trainWholeErr_30NP) \\\n",
    "            ,np.mean(trainWholeErr_60NP) \\\n",
    "            ,np.mean(trainWholeErr_90NP) \\\n",
    "            ,np.mean(trainWholeErr_120NP) \\\n",
    "            ,np.mean(trainWholeErr_150NP) \\\n",
    "            ,np.mean(trainWholeErr_180NP) \\\n",
    "            ,np.mean(trainWholeErr_210NP) \\\n",
    "            ,np.mean(trainWholeErr_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_func(value, tick_number):\n",
    "    tempTick = (value*30) + 30\n",
    "    ret = \"%d\"%(tempTick)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Training\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(trainWholeErrMean,label = \"TS = %d\"%(modelTS))\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute errors for all testing trajectories\n",
    "testDataWholeErrors = []\n",
    "missedTrajCount = 0\n",
    "# for vesselDataSource in vesselDataSourcesTest[0:1]:\n",
    "for vesselDataSource in vesselDataSourcesTest[0:]:\n",
    "    print(\"Taking data from:%s\"%vesselDataSource.srcDir)\n",
    "    mMSIListFile = vesselDataSource.srcDir + 'TrainTrajCount.txt'\n",
    "    mMSIList = [line.rstrip('\\n') for line in open(mMSIListFile)]\n",
    "#     for traj in mMSIList[0:1]:\n",
    "    for traj in mMSIList[0:]:\n",
    "        numTraj = traj.split(\"_\")[-1]\n",
    "        trajAffix = traj[0:-len(numTraj)]\n",
    "        for num in range(int(numTraj)):\n",
    "            trajName = trajAffix + str(num) +'.csv'\n",
    "            #TODO:.replace statement can be removed in the future\n",
    "            testDataWholeErrors.append(get_error_for_traj(trajName.replace(\"jcharla\",\"jagir\"),vesselDataSource.type))\n",
    "\n",
    "testWholeErr_30 = []\n",
    "testWholeErr_60 = []\n",
    "testWholeErr_90 = []\n",
    "testWholeErr_120 = []\n",
    "testWholeErr_150 = []\n",
    "testWholeErr_180 = []\n",
    "testWholeErr_210 = []\n",
    "testWholeErr_240 = []\n",
    "\n",
    "testWholeErr_n_30 = [testWholeErr_30 \\\n",
    ", testWholeErr_60 \\\n",
    ", testWholeErr_90 \\\n",
    ", testWholeErr_120 \\\n",
    ", testWholeErr_150 \\\n",
    ", testWholeErr_180 \\\n",
    ", testWholeErr_210 \\\n",
    ", testWholeErr_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in testDataWholeErrors:\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            testWholeErr_n_30[n_30].append(trajErr[n_30])\n",
    "            \n",
    "testWholeErr_30NP = np.array(testWholeErr_30)\n",
    "testWholeErr_60NP = np.array(testWholeErr_60)\n",
    "testWholeErr_90NP = np.array(testWholeErr_90)\n",
    "testWholeErr_120NP = np.array(testWholeErr_120)\n",
    "testWholeErr_150NP = np.array(testWholeErr_150)\n",
    "testWholeErr_180NP = np.array(testWholeErr_180)\n",
    "testWholeErr_210NP = np.array(testWholeErr_210)\n",
    "testWholeErr_240NP = np.array(testWholeErr_240)\n",
    "\n",
    "testWholeErrMean = [np.mean(testWholeErr_30NP) \\\n",
    "            ,np.mean(testWholeErr_60NP) \\\n",
    "            ,np.mean(testWholeErr_90NP) \\\n",
    "            ,np.mean(testWholeErr_120NP) \\\n",
    "            ,np.mean(testWholeErr_150NP) \\\n",
    "            ,np.mean(testWholeErr_180NP) \\\n",
    "            ,np.mean(testWholeErr_210NP) \\\n",
    "            ,np.mean(testWholeErr_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Testing\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(testWholeErrMean,label = \"TS = %d\"%(modelTS))\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the error results\n",
    "dataToStore = modelDir + \"trainWholeErr_30NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_30NP)\n",
    "dataToStore = modelDir + \"trainWholeErr_60NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_60NP)\n",
    "dataToStore = modelDir + \"trainWholeErr_90NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_90NP)\n",
    "dataToStore = modelDir + \"trainWholeErr_120NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_120NP)\n",
    "dataToStore = modelDir + \"trainWholeErr_150NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_150NP)\n",
    "dataToStore = modelDir + \"trainWholeErr_180NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_180NP)\n",
    "dataToStore = modelDir + \"trainWholeErr_210NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_210NP)\n",
    "dataToStore = modelDir + \"trainWholeErr_240NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_240NP)\n",
    "\n",
    "dataToStore = modelDir + \"testWholeErr_30NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_30NP)\n",
    "dataToStore = modelDir + \"testWholeErr_60NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_60NP)\n",
    "dataToStore = modelDir + \"testWholeErr_90NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_90NP)\n",
    "dataToStore = modelDir + \"testWholeErr_120NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_120NP)\n",
    "dataToStore = modelDir + \"testWholeErr_150NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_150NP)\n",
    "dataToStore = modelDir + \"testWholeErr_180NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_180NP)\n",
    "dataToStore = modelDir + \"testWholeErr_210NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_210NP)\n",
    "dataToStore = modelDir + \"testWholeErr_240NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_240NP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
